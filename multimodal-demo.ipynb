{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780832a6",
   "metadata": {},
   "source": [
    "# Generative AI and Multi-Modal Agents in AWS: The Key to Unlocking New Value in Financial Markets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099f3da",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413e86e",
   "metadata": {},
   "source": [
    "### Install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile demo-requirements.txt\n",
    "anthropic==0.2.10\n",
    "boto3\n",
    "langchain==0.0.309\n",
    "langchain_experimental\n",
    "PyAthena[SQLAlchemy]==2.25.2\n",
    "sqlalchemy==1.4.47\n",
    "pandas<2.0.0\n",
    "numpy==1.22\n",
    "nest-asyncio==1.5.5\n",
    "PyPortfolioOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc2c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -r demo-requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c2343",
   "metadata": {},
   "source": [
    "Modify the region. Make sure it's a region that supports Kendra. https://aws.amazon.com/about-aws/whats-new/2021/06/amazon-kendra-adds-support-for-new-aws-regions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39e105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = 'us-east-1' ## Change to region name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f12e0",
   "metadata": {},
   "source": [
    "Use one of the following methods to get Anthropic API Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c8e7c",
   "metadata": {},
   "source": [
    "Method 1: Provide API key to Anthropic foundation model. Recommend using AWS Secrets Manager to manage your API key. Please follow the instructions to store your key in AWS Secret Manager. https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_secret.html Uncomment the code below if you use this method. Modify the secret_name accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd80d3",
   "metadata": {},
   "source": [
    "### Get resource names\n",
    "\n",
    "The CloudFormation stack created the infrastructure for this application, such as S3 buckets and Lambda functions. We will get the names/ids of these resources. \n",
    "\n",
    "Modify the CFN_STACK_NAME based on the name you used when creating the CloudFormation stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f4d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modify the stack name to match the name of your CloudFormation stack\n",
    "\n",
    "CFN_STACK_NAME = \"mmfsi\" ## Change to name of cloudformation template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867e2ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import List\n",
    "\n",
    "\n",
    "stacks = boto3.client('cloudformation',region_name=region).list_stacks()\n",
    "stack_found = CFN_STACK_NAME in [stack['StackName'] for stack in stacks['StackSummaries']]\n",
    "\n",
    "\n",
    "def get_cfn_outputs(stackname: str) -> List:\n",
    "    cfn = boto3.client('cloudformation', region_name=region)\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "# this function extracts the bucket name from S3 uri.\n",
    "# For example, the bucket name is 'my_bucket' based on the URI \"s3://my_bucket/\"\n",
    "def get_bucket_name(s3_uri):\n",
    "    bucket_name = s3_uri.split(\"/\")[2]\n",
    "    return bucket_name\n",
    "\n",
    "\n",
    "if stack_found is True:\n",
    "    outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
    "    glue_db_name = outputs['stockpricesdb']\n",
    "    kendra_index_id = outputs['KendraIndexId']\n",
    "    audio_transcripts_source_bucket = get_bucket_name(outputs['AudioSourceBucket'])\n",
    "    textract_source_bucket = get_bucket_name(outputs['PDFSourceBucket'])\n",
    "    query_staging_bucket = get_bucket_name(outputs['QueryStagingBucket'])\n",
    "    stock_data_source_bucket = get_bucket_name(outputs['StockDataSourceBucket'])\n",
    "    multimodal_output_bucket = get_bucket_name(outputs['MultimodalOutputBucket'])\n",
    "    print (f\"glue_db_name is {glue_db_name}.\")\n",
    "    print (f\"kendra_index_id is {kendra_index_id}.\")\n",
    "    print (f\"audio_transcripts_source_bucket is {audio_transcripts_source_bucket}.\")\n",
    "    print (f\"textract_source_bucket is {textract_source_bucket}.\")\n",
    "    print (f\"query_staging_bucket is {query_staging_bucket}.\")\n",
    "    print (f\"stock_data_source_bucket is {stock_data_source_bucket}.\")\n",
    "    print (f\"multimodal_output_bucket is {multimodal_output_bucket}.\")\n",
    "else:\n",
    "    print(\"Recheck our cloudformation stack name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db44149-8671-49da-bca1-fbda96c2821c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "param={}\n",
    "param['db']=glue_db_name\n",
    "param['query_bucket']=query_staging_bucket\n",
    "param['region']=region\n",
    "param['kendra_id']=kendra_index_id\n",
    "\n",
    "#Store parameters in json file\n",
    "with open('param.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(param, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa51f4",
   "metadata": {},
   "source": [
    "### Upload the files to the S3 buckets\n",
    "\n",
    "Next, we will upload the documents to the S3 buckets created by CloudFormation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead09c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print (f'Uploaded {file_name} to S3 bucket {bucket}.')\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "upload_file('files/Amazon-10K-2022-EarningsReport.pdf', textract_source_bucket)\n",
    "upload_file('files/Amazon-10Q-Q1-2023-QuaterlyEarningsReport.pdf', textract_source_bucket)\n",
    "upload_file('files/Amazon-Quarterly-Earnings-Report-Q1-2023-Full-Call-v1.mp3', audio_transcripts_source_bucket)\n",
    "upload_file('./files/stock_prices.csv', stock_data_source_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fb714",
   "metadata": {},
   "source": [
    "### Create a table in Amazon Athena\n",
    "We will store the stock data in Athena for querying. The stock data has the following format:\n",
    "\n",
    "date|AAAA|FF|BBB|ZZZZ|...\n",
    "\n",
    "AAAA, etc., are fake stock symbols.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30655566",
   "metadata": {},
   "source": [
    "First, drop the existing table as we will create a new one. Copy and past the query in the Athena Query Editor.\n",
    "\n",
    "```\n",
    "DROP TABLE `stock_prices`;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc09fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f\"stock_data_source_bucket is {stock_data_source_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09fd7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('athena',region_name=region)\n",
    "\n",
    "def query_athena(query):\n",
    "    response = client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': 'blog-stock-prices-db'\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': f's3://{query_staging_bucket}/',\n",
    "        },\n",
    "        WorkGroup='primary'\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034280c0",
   "metadata": {},
   "source": [
    "Replace the existing table with a new table that imports the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abaaef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_table_query='DROP TABLE `stock_prices`;'\n",
    "query_athena(drop_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64893a10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_table_query=f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS `blog-stock-prices-db`.stock_prices ( \n",
    "    date string, \n",
    "    AAAA double, \n",
    "    FF double, \n",
    "    BBBB double, \n",
    "    ZZZZ double, \n",
    "    GG double, \n",
    "    DDD double, \n",
    "    WWW double, \n",
    "    CCC double, \n",
    "    GGMM double, \n",
    "    TTT double, \n",
    "    UUU double, \n",
    "    SSSS double, \n",
    "    XXX double, \n",
    "    RRR double, \n",
    "    YYY double, \n",
    "    MM double, \n",
    "    PPP double, \n",
    "    JJJ double, \n",
    "    SSXX double\n",
    ") \n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES ('separatorChar' = ',', 'quoteChar' = '\\\\\\\"', 'escapeChar' = '\\\\\\\\')\n",
    "LOCATION 's3://{stock_data_source_bucket}/'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\"\n",
    "query_athena(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16acce",
   "metadata": {},
   "source": [
    "# Create tools\n",
    "\n",
    "In the following section, we will define various tools for the agent. The tools include:\n",
    "    \n",
    "* Stocks Querying Tool to query S&P stocks data using Amazon Athena and SQL Alchemy.\n",
    "* Portfolio Optimization Tool that builds a portfolio based on the chosen stocks.\n",
    "* Financial Information Lookup Tool to search for financial earnings information stored in multi-page pdf files using Amazon Kendra.\n",
    "* Python Calculation Tool that can be used to do mathematical calculations.\n",
    "* Sentiment Analysis Tool to identify and score sentiments on a topic using Amazon Comprehend.\n",
    "* Detect Phrases Tool to find key phrases in recent quarterly reports using Amazon Comprehend.\n",
    "* Text Extraction Tool to convert pdf version of quarterly reports to text files using Amazon Textract.\n",
    "* Transcribe Audio Tool to convert audio recordings to text files using Amazon Transcribe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d74ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import PromptTemplate,SQLDatabase, LLMChain\n",
    "from langchain_experimental.sql.base import SQLDatabaseChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_experimental.sql.base import SQLDatabaseChain\n",
    "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
    "from langchain.chains import APIChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.base import StructuredTool\n",
    "from typing import Optional\n",
    "from langchain.tools import BaseTool\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain import LLMMathChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories import DynamoDBChatMessageHistory\n",
    "\n",
    "from typing import Dict\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b914e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to an LLM\n",
    "\n",
    "# Create the bedrock runtime to invoke LLM\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "config = Config(\n",
    "    read_timeout=120,\n",
    "    retries = dict(\n",
    "        max_attempts = 5 ## Handle retries\n",
    "    )\n",
    ")\n",
    "\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name='us-east-1')#,config=config)\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "inference_modifier = {'max_tokens_to_sample':350, \n",
    "                      \"temperature\":0.0,\n",
    "                      \"top_k\":50,                        \n",
    "                     }\n",
    "llm = Bedrock(model_id='anthropic.claude-v2', client=bedrock_runtime, model_kwargs = inference_modifier,\n",
    "              streaming=True,  # Toggle this to turn streaming on or off\n",
    "              callbacks=[StreamingStdOutCallbackHandler() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249f781-cdea-4359-a0d2-1ef5d8acf8de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify the following parameters as needed\n",
    "table = 'stock_prices'\n",
    "\n",
    "connathena=f\"athena.{region}.amazonaws.com\" \n",
    "portathena='443' #Update, if port is different\n",
    "schemaathena=glue_db_name #from user defined params\n",
    "s3stagingathena=f's3://{query_staging_bucket}/athenaresults/'#from cfn params\n",
    "wkgrpathena='primary'#Update, if workgroup is different\n",
    "\n",
    "##  Create the athena connection string\n",
    "connection_string = f\"awsathena+rest://@{connathena}:{portathena}/{schemaathena}?s3_staging_dir={s3stagingathena}&work_group={wkgrpathena}\"\n",
    "\n",
    "##  Create the athena  SQLAlchemy engine\n",
    "engine_athena = create_engine(connection_string, echo=False)\n",
    "dbathena = SQLDatabase(engine_athena)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b7657",
   "metadata": {},
   "source": [
    "## Create Stocks Querying Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9911ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    sql_template = \"\"\"\n",
    "First, you understand the question being asked of you, then you perform one of two task depending on the information provided to you:\n",
    "1. Assuming a role of a SQL developer, generate SQL statements. Only provide the sql statements alone and no additional text.\n",
    "2. Assuming the role of an assistant, provide a clear and direct answer to the question.\n",
    "\n",
    "Assistant: \n",
    "I understand my role and task. I would understand the question and either provide a sql statement or a natural language answer depending on the information I am provided.\n",
    "\n",
    "Human:    \n",
    "Here are certain nuances to questions that you should pay attention to:\n",
    "1. If a question ask for \"closing prices\", it should be the last price at which a stock trades for the given time period. For example if the time period in the question is 2020, then the closing price is the price for the last available date in 2020.\n",
    "\n",
    "Assistant: \n",
    "Thank you for the clarification. I would treat any questions with \"closing prices\" as the last price for the given date period.\n",
    "\n",
    "Human:    \n",
    "Here is a schema of a table:\n",
    "<schema>\n",
    "{table_info}\n",
    "</schema>       \n",
    "\n",
    "When providing your response:   \n",
    "1. If \"SQLResult\" is not provided, your response should only contain the SQL statement as additional non-sql text would cause Athena to throw an error.\n",
    "2. If \"SQLResult\" is provided, your response should be based off the context in \"SQLResult\" as a financial expert.\n",
    "\n",
    "Assistant:   \n",
    "Yes, if there is not a SQLResult information provided that tells me that I would be only providing SQL statements alone as additional text would cause Amazon Athena to throw an error.\n",
    "However, if SQLResult is provided to me, I would only provide answers to the question based off the SQLResult context after understanding the question from a financial experts view.\n",
    "\n",
    "Human:\n",
    "Question: {input}\n",
    "SQLQuery:\"\"\"\n",
    "    inference_modifier = {'max_tokens_to_sample':350, \n",
    "                      \"temperature\":0.0,\n",
    "                      # \"top_k\":50,                        \n",
    "                     }\n",
    "    llm_query = Bedrock(model_id='anthropic.claude-instant-v1', client=bedrock_runtime, model_kwargs = inference_modifier,\n",
    "              streaming=True,  # Toggle this to turn streaming on or off\n",
    "              callbacks=[StreamingStdOutCallbackHandler() ])\n",
    "    PROMPT_sql = PromptTemplate(\n",
    "        input_variables=[\"input\", \"table_info\"], template=sql_template\n",
    "    )\n",
    "    \n",
    "    db_chain = SQLDatabaseChain.from_llm(llm_query, dbathena, prompt=PROMPT_sql, verbose=True,\n",
    "                                         return_intermediate_steps=False)\n",
    "    response=db_chain(query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a2b39",
   "metadata": {},
   "source": [
    "Test the run_query tool with a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7d8ef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the run_query tool\n",
    "run_query('AAAA WWW DDD closing price 2018?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3384c1",
   "metadata": {},
   "source": [
    "## Create Portfolio Optimization Tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d9e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.base import StructuredTool\n",
    "from typing import Optional\n",
    "from langchain.tools import BaseTool\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class OptimizePortfolio(BaseTool):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    name = \"Portfolio Optimization Tool\"\n",
    "    description = \"\"\"\n",
    "use this tool when you need to build optimal portfolio. \n",
    "The output results tell you the allocation of your money on each stock.\n",
    "No need to pass stock prices to this tool.\n",
    "This tool only accept stock tickers in this format \"stock_ls\":[\"stock tickers\"]. Example \"stock_ls\":[\"xyz\",\"abc\"]     \n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    def _run(self, *args, **kwargs):\n",
    "        print(kwargs)\n",
    "        import boto3\n",
    "        import pandas as pd\n",
    "        from pyathena import connect\n",
    "        \n",
    "        # Establish connection to Athena\n",
    "        session = boto3.Session(region_name=region)\n",
    "        athena_client = session.client('athena')\n",
    "        stock_ls = kwargs.get('stock_ls', None)\n",
    "        if stock_ls is None:\n",
    "              raise ValueError(\"Please provide stock list as stock_ls\")\n",
    "        # Execute query\n",
    "        query = f'SELECT * from \"blog-stock-prices-db\".\"stock_prices\"'  \n",
    "        cursor = connect(s3_staging_dir=f's3://{query_staging_bucket}/athenaresults/', region_name=region).cursor()\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch results\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Convert to Pandas DataFrame\n",
    "        df = pd.DataFrame(rows, columns=[column[0] for column in cursor.description])\n",
    "\n",
    "        # Filter data to use the designated list of stocks\n",
    "        stock_ls = [x.lower() for x in stock_ls]\n",
    "        stock_ls.append('date')        \n",
    "        df = df[stock_ls]\n",
    "\n",
    "        # Set \"Date\" as the index and parse it as a datetime object\n",
    "        df.set_index(\"date\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index, format = '%Y-%m-%d')\n",
    "        \n",
    "        mu = expected_returns.mean_historical_return(df)\n",
    "        S = risk_models.sample_cov(df)\n",
    "\n",
    "        # Optimize for maximal Sharpe ratio\n",
    "        ef = EfficientFrontier(mu, S)\n",
    "        weights = ef.max_sharpe()\n",
    "        ef.portfolio_performance(verbose=True)\n",
    "\n",
    "        cleaned_weights = ef.clean_weights()\n",
    "        ef.portfolio_performance(verbose=True)\n",
    "        #Finally, let’s convert the weights into actual allocations values (i.e., how many of each stock to buy). For our allocation, let’s consider an investment amount of $100,000:\n",
    "\n",
    "        from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices\n",
    "        latest_prices = get_latest_prices(df)\n",
    "        da = DiscreteAllocation(weights, latest_prices, total_portfolio_value=10000)\n",
    "        allocation, leftover = da.greedy_portfolio()       \n",
    "        results=str(dict(cleaned_weights)).replace('{',\"\").replace('}',\"\")\n",
    "        return f\"These are the optimized portfolio {results}\"\n",
    "\n",
    "    def _arun(self, stock_ls: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd5485",
   "metadata": {},
   "source": [
    "# Define tools that use Amazon Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576aa82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SentimentAnalysis(inputString):\n",
    "    # print(inputString)\n",
    "    lambda_client = boto3.client('lambda', region_name=region)\n",
    "    lambda_payload = {\"inputString:\"+inputString}\n",
    "    response=lambda_client.invoke(FunctionName='FSI-SentimentDetecttion',\n",
    "                        InvocationType='RequestResponse',\n",
    "                     Payload=json.dumps(inputString))\n",
    "    output=json.loads(response['Payload'].read().decode())\n",
    "    return output['body']\n",
    "\n",
    "def DetectKeyPhrases(inputString):\n",
    "    # print(inputString)\n",
    "    lambda_client = boto3.client('lambda', region_name=region)\n",
    "    lambda_payload = {\"inputString:\"+inputString}\n",
    "    response=lambda_client.invoke(FunctionName='FSI-KeyPhrasesDetection',\n",
    "                        InvocationType='RequestResponse',\n",
    "                     Payload=json.dumps(inputString))\n",
    "    output=json.loads(response['Payload'].read().decode())\n",
    "    return output['body']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139143c",
   "metadata": {},
   "source": [
    "# Define tool that uses AWS Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bb95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def IntiateTextExtractProcessing(inputString):\n",
    "    print(inputString)\n",
    "    lambda_client = boto3.client('lambda', region_name=region)\n",
    "    lambda_payload = {\"inputString:\"+inputString}\n",
    "    response=lambda_client.invoke(FunctionName='FSI-TextractAsyncInvocationFunction',\n",
    "                        InvocationType='RequestResponse',\n",
    "                     Payload=json.dumps(inputString))\n",
    "    print(response['Payload'].read())\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b0bda",
   "metadata": {},
   "source": [
    "# Define tool that uses Amazon Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab006b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TranscribeAudio(inputString):\n",
    "    print(inputString)\n",
    "    lambda_client = boto3.client('lambda', region_name=region)\n",
    "    lambda_payload = {\"inputString:\"+inputString}\n",
    "    response=lambda_client.invoke(FunctionName='FSI-Transcribe',\n",
    "                        InvocationType='RequestResponse',\n",
    "                     Payload=json.dumps(inputString))\n",
    "    print(response['Payload'].read())\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa78898",
   "metadata": {},
   "source": [
    "# Create Financial Information Lookup Tool \n",
    "\n",
    "Kendra helps you find faster with intelligent enterprise search powered by machine learning. We will use Kendra to find answers in *Amazon-10K-2022-EarningsReport.pdf*, *Amazon-10Q-Q1-2023-QuaterlyEarningsReport.pdf* and trasncriptions of *Amazon-Quarterly-Earnings-Report-Q1-2023-Full-Call-v1.mp3*.\n",
    "\n",
    "Sample question: “What’s Amazon’s unearned revenue from AWS and Prime memberships as of December 31, 2022? What is the profitability ratio as of December 31, 2022\"\n",
    "\n",
    "First, we to do two pre-processing steps to extract the text of the PDF files. This process takes about 15 minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8656a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IntiateTextExtractProcessing('process')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bf385",
   "metadata": {},
   "source": [
    "Next, we need to transcribe the .mpd audio file. This also takes about 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084cc63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TranscribeAudio('process')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706556c",
   "metadata": {},
   "source": [
    "The above process takes about 15 minutes. The code below will check the S3 bucket every minute. If the process is completed, the code will show \"The S3 bucket contains all the necessary output.\", and please proceed to the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de359b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_s3_files_using_client(bucket):\n",
    "    \"\"\"\n",
    "    This functions list all files in s3 bucket.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    s3_client=boto3.client(\"s3\")\n",
    "    bucket_name=bucket\n",
    "    response=s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    files=response.get(\"Contents\")\n",
    "    output_list=[]\n",
    "    for file in files:\n",
    "        output_list.append(file['Key'])\n",
    "    return output_list\n",
    "\n",
    "correct_list = [\n",
    "    'audiooutputs/Amazon-Quarterly-Earnings-Report-Q1-2023-Full-Call-v1.txt',\n",
    "    'pdfoutputs/Amazon-10K-2022-EarningsReport.txt',\n",
    "    'pdfoutputs/Amazon-10Q-Q1-2023-QuaterlyEarningsReport.txt'\n",
    "]\n",
    "\n",
    "\n",
    "s3_files_list=list_s3_files_using_client(multimodal_output_bucket)\n",
    "check =  all(item in s3_files_list for item in correct_list)\n",
    " \n",
    "while not check:\n",
    "    time.sleep(30)\n",
    "    s3_files_list=list_s3_files_using_client(multimodal_output_bucket)\n",
    "    check =  all(item in s3_files_list for item in correct_list)\n",
    "else:\n",
    "    print ('The S3 bucket contains all the necessary output. Please proceed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffdc41",
   "metadata": {},
   "source": [
    "Next, we are going to sync Amazon Kendra with the text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d8f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import List\n",
    "\n",
    "\n",
    "stacks = boto3.client('cloudformation', region_name=region).list_stacks()\n",
    "stack_found = CFN_STACK_NAME in [stack['StackName'] for stack in stacks['StackSummaries']]\n",
    "\n",
    "\n",
    "def get_cfn_outputs(stackname: str) -> List:\n",
    "    cfn = boto3.client('cloudformation', region_name=region)\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "# this function extracts the bucket name from S3 uri.\n",
    "# For example, the bucket name is 'my_bucket' based on the URI \"s3://my_bucket/\"\n",
    "def get_bucket_name(s3_uri):\n",
    "    bucket_name = s3_uri.split(\"/\")[2]\n",
    "    return bucket_name\n",
    "\n",
    "\n",
    "if stack_found is True:\n",
    "    outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
    "    glue_db_name = outputs['stockpricesdb']\n",
    "    kendra_index_id = outputs['KendraIndexId']\n",
    "    kendra_data_source_id = outputs['KendraDataSourceId']\n",
    "    audio_transcripts_source_bucket = get_bucket_name(outputs['AudioSourceBucket'])\n",
    "    textract_source_bucket = get_bucket_name(outputs['PDFSourceBucket'])\n",
    "    query_staging_bucket = get_bucket_name(outputs['QueryStagingBucket'])\n",
    "    stock_data_source_bucket = get_bucket_name(outputs['StockDataSourceBucket'])\n",
    "    multimodal_output_bucket = get_bucket_name(outputs['MultimodalOutputBucket'])\n",
    "    \n",
    "    print (f\"glue_db_name is {glue_db_name}.\")\n",
    "    print (f\"kendra_index_id is {kendra_index_id}.\")\n",
    "    print (f\"kendra_data_source_id is {kendra_data_source_id}.\")\n",
    "    print (f\"audio_transcripts_source_bucket is {audio_transcripts_source_bucket}.\")\n",
    "    print (f\"textract_source_bucket is {textract_source_bucket}.\")\n",
    "    print (f\"query_staging_bucket is {query_staging_bucket}.\")\n",
    "    print (f\"stock_data_source_bucket is {stock_data_source_bucket}.\")\n",
    "    print (f\"multimodal_output_bucket is {multimodal_output_bucket}.\")\n",
    "else:\n",
    "    print(\"Recheck our cloudformation stack name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90287dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kendra_client = boto3.client(\"kendra\")\n",
    "\n",
    "sync_response = kendra_client.start_data_source_sync_job(\n",
    "    Id = kendra_data_source_id,\n",
    "    IndexId = kendra_index_id\n",
    ")\n",
    "\n",
    "print(\"Wait for the data source to sync with the index.\")\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "while True:\n",
    "    jobs = kendra_client.list_data_source_sync_jobs(\n",
    "        Id = kendra_data_source_id,\n",
    "        IndexId = kendra_index_id\n",
    "    )\n",
    "\n",
    "    status = jobs[\"History\"][0][\"Status\"]\n",
    "    print(\" Syncing data source. Status: \"+status)\n",
    "    \n",
    "    if status in ['FAILED','SUCCEEDED','INCOMPLETE','ABORTED']:\n",
    "        break\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141a834-7ad8-4ddf-8907-20f0c5c3d23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "def build_chain():\n",
    "    inference_modifier = {'max_tokens_to_sample':450, \n",
    "                      \"temperature\":0.1,\n",
    "                      \"top_k\":50,                        \n",
    "                     }\n",
    "    llm_retrieve = Bedrock(model_id='anthropic.claude-instant-v1', client=bedrock_runtime, model_kwargs = inference_modifier,\n",
    "              streaming=True,  # Toggle this to turn streaming on or off\n",
    "              callbacks=[StreamingStdOutCallbackHandler() ])\n",
    "       \n",
    "    retriever = AmazonKendraRetriever(index_id=kendra_index_id,region_name=region, top_k=10)\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "This is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides specific details from its context but limits it to 240 tokens.\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "Net income can be net loss. If the value is in paranthesis, it means it is a loss and hence a negative value. For example, (1000) means -1000.\n",
    "\n",
    "Assistant: OK, got it, I'll be a talkative truthful AI assistant.\n",
    "\n",
    "Human: Here are a few documents: \n",
    "{context}\n",
    "\n",
    "Based on the above documents, provide a straightforward answer for {question}. \n",
    "Answer \"don't know\" if not present in the document.\"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    chain_type_kwargs = {\"prompt\": PROMPT}\n",
    " \n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm_retrieve, \n",
    "        chain_type=\"stuff\", \n",
    "        retriever=retriever, \n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "\n",
    "def run_chain(prompt: str, history=[]):\n",
    "    chain = build_chain()\n",
    "    result = chain(prompt)\n",
    "    return result['result']\n",
    "    # return {\n",
    "    #     \"answer\": result['result'],\n",
    "    #     \"source_documents\": result[\"source_documents\"][0].metadata['source']\n",
    "    # }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb6f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test the tool\n",
    "result = run_chain(\"What's Amazon's total unearned revenue in 2022?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a0cb9",
   "metadata": {},
   "source": [
    "# Define a toolkit\n",
    "\n",
    "Now that we have defined all the individual tools, we will put together a toolkit for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5c76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents.tools import Tool\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Stock Querying Tool\",\n",
    "        func=run_query,\n",
    "        description=\"\"\"\n",
    "        Useful for when you need to answer questions about stocks. It only has information about stocks.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    OptimizePortfolio(),\n",
    "    Tool(\n",
    "        name=\"Financial Information Lookup Tool\",\n",
    "        func=run_chain,\n",
    "        description=\"\"\"\n",
    "        Useful for when you need to look up financial information using kendra. \n",
    "        \"\"\"\n",
    "    ),\n",
    "    PythonREPLTool(),\n",
    "    Tool(\n",
    "        name=\"Sentiment Analysis Tool\",\n",
    "        func=SentimentAnalysis,\n",
    "        description=\"\"\"\n",
    "        Useful for when you need to analyze the sentiment of a topic, such as \"Return to Office\".\n",
    "        \"\"\"\n",
    "    ),\n",
    "     Tool(\n",
    "        name=\"Detect Phrases Tool\",\n",
    "        func=DetectKeyPhrases,\n",
    "        description=\"\"\"\n",
    "        Useful for when you need to detect key phrases in recent quaterly reports.\n",
    "        \"\"\"\n",
    "    ),\n",
    "     Tool(\n",
    "        name=\"Text Extraction Tool\",\n",
    "        func=IntiateTextExtractProcessing,\n",
    "        description=\"\"\"\n",
    "        Useful for when you need to trigger conversion of  pdf version of quaterly reports to text files using amazon textextract\n",
    "        \"\"\"\n",
    "    ),\n",
    "     Tool(\n",
    "        name=\"Transcribe Audio Tool\",\n",
    "        func=TranscribeAudio,\n",
    "        description=\"\"\"\n",
    "        Useful for when you need to convert audio recordings of earnings calls from audio to text format using Amazon Transcribe\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e4d42",
   "metadata": {},
   "source": [
    "Modify the prompt template to provide the agent guidance on how to use the tools. The current template of prompt is based on Anthropic Claude2. If you choose to use a different foundation model, please try tweaking the template. It's just like you would convey the same idea in slightly different ways to different people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b46079-226c-487e-b062-706651b5c0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combo_template = \"\"\"\\n\\nHuman:\n",
    "You are a Minimization Solutionist with a set of tools at your disposal.\n",
    "\n",
    "You would be presented with a problem. First understand the problem and devise a plan to solve the problem. \n",
    "Please output the plan starting with the header 'Plan:' and then followed by a numbered list of steps.        \n",
    "Ensure the plan has the minimum amount of steps needed to solve the problem. Do not include unnecessary steps.\n",
    "\n",
    "<instructions>\n",
    "These are guidance on when to use a tool to solve a task, follow them strictly:\n",
    "1. For the tool that specifically focuses on stock price data, use \"Stock Query Tool\".    \n",
    "2. For financial information lookup that covers various financial data like company's finance, performance or any other information pertaining a company beyond stocks, use the \"Financial Data Explorer Tool\". Ask specific questions using this tool as it is your knowledge database. Refrain from asking question like \"look up 10K filings\" instead a more specific question like \"what is the revenue for this company\". \n",
    "3. When you need to find key phrases in a report, use the \"Detect Phrases Tool\" to get the information about all key phrases and respond with key phrases relavent to the question. \n",
    "4. When you need to provide an optimized stock portfolio based on stock names, use Portfolio Optimization Tool. The output is the percent of fund you should spend on each stock. This tool only takes stock ticker as input and not stock prices, for example [\"EWR\",\"JHT\"].\n",
    "5. Please use the PythonREPLTool exclusively for calculations, refrain from utilizing 'print' statements for output. Use this too only when needed, most times its unnecessary.\n",
    "6. When you need to analyze sentiment of a topic, use \"Sentiment Analysis Tool\".\n",
    "</instructions>\\n\\nAssistant:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0033e-7b63-495d-b865-4e22de5ae72c",
   "metadata": {},
   "source": [
    "Adding a conversation history element to the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af3371-9bb8-4fe4-94f1-71800b529408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history_table = 'SessionTable' # Name of dynamoDB Table for storing converstaions (prompts and answers)\n",
    "  \n",
    "chat_session_id = '0'\n",
    "  \n",
    "if chat_session_id == '0' :\n",
    "    chat_session_id = str(uuid.uuid4())\n",
    "\n",
    "print (chat_session_id)\n",
    "\n",
    "chat_history_memory = DynamoDBChatMessageHistory(table_name=chat_history_table, session_id=chat_session_id)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=chat_history_memory, return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914f093",
   "metadata": {},
   "source": [
    "The agent has two parts, a planner and an executor. The planner sets up the steps necessary to answer the questions, and the executor carries out the plans using the tools in the toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47e030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_modifier = {'max_tokens_to_sample':350, \n",
    "                      \"temperature\":0.1,\n",
    "                      # \"top_k\":50,                        \n",
    "                     }\n",
    "llm = Bedrock(model_id='anthropic.claude-v2', client=bedrock_runtime, model_kwargs = inference_modifier,\n",
    "              streaming=True,  # Toggle this to turn streaming on or off\n",
    "              callbacks=[StreamingStdOutCallbackHandler() ])\n",
    "planner = load_chat_planner(llm)\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(combo_template)\n",
    "human_message_prompt = planner.llm_chain.prompt.messages[1]\n",
    "planner.llm_chain.prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "executor = load_agent_executor(llm, tools, verbose=False)\n",
    "agent = PlanAndExecute(planner=planner, executor=executor, verbose=False, max_iterations=1, return_intermediate_steps=True)#, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380dd65",
   "metadata": {},
   "source": [
    "# Ask the agent questions!\n",
    "\n",
    "Please note that the results are non-deterministic becuase of the nature of Large Languaeg Models (LLM), so what you get each time can be different, and they can also be different from what are in the blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf3652-e546-4f16-913a-644dd8a0a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent(\"What are the closing prices of stocks AAAA, WWW, DDD in year 2018? Can you build an optimized portfolio using these three stocks? Please provide answers to both questions.\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73665b-76d7-4a51-beb4-d9c8f8317d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = agent.run(\"What are the Amazon's top priorities? How do you see the company's competitive landscape evolving? What are the biggest risks facing the company? Please limit your answers to 5 sentences.\")\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
